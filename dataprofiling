from pyspark.sql import SparkSession
from pyspark.sql.functions import col, isnan, when, count, countDistinct, mean, expr, trim
from pyspark.sql.types import NumericType
from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler, RobustScaler

spark = SparkSession.builder.appName("Data_preprocessing").getOrCreate()

data = [
    (1, " Alice ", 34, 70000),
    (2, "Bob", None, 60000),
    (3, "charlie", 29, None),
    (4, "David", 45, 80000),
    (5, "Eve", None, 55000)
]

df = spark.createDataFrame(data, schema=["id", "name", "age", "salary"])


def data_profiling(df):
    print("=== Schema ===")
    df.printSchema()
    
    print("=== Sample Data ===")
    df.show(truncate=False)
    
    print(f"Total records: {df.count()}")
    
    print("=== Null Count per Column ===")
    exprs = []
    for field in df.schema.fields:
        c = field.name
        if isinstance(field.dataType, NumericType):
            exprs.append(count(when(col(c).isNull() | isnan(col(c)), c)).alias(c))
        else:
            exprs.append(count(when(col(c).isNull(), c)).alias(c))
    df.select(exprs).show(truncate=False)
    
    print("=== Distinct Count per Column ===")
    df.agg(*[countDistinct(c).alias(c) for c in df.columns]).show(truncate=False)
    
    print("=== Basic Statistics ===")
    df.describe().show(truncate=False)


def data_cleaning(df):
    # Impute missing values with mean (age) and median (salary)
    mean_age = df.select(mean(col("age"))).first()[0]
    median_salary = df.approxQuantile("salary", [0.5], 0.01)[0]
    df = df.withColumn("age", when(col("age").isNull(), mean_age).otherwise(col("age")))
    df = df.withColumn("salary", when(col("salary").isNull(), median_salary).otherwise(col("salary")))

    print("DataFrame after imputing missing values:")
    df.show(truncate=False)

    # Remove duplicates
    df = df.dropDuplicates()
    print("DataFrame after removing duplicates:")
    df.show(truncate=False)

    # Clean 'name' column: trim and capitalize first letter
    df = df.withColumn("name", expr("initcap(trim(name))"))
    print("DataFrame after cleaning 'name' column:")
    df.show(truncate=False)

    # Outlier detection and removal on 'salary' using IQR
    quantiles = df.approxQuantile("salary", [0.25, 0.75], 0.01)
    Q1, Q3 = quantiles
    IQR = Q3 - Q1
    df = df.filter((col("salary") >= (Q1 - 1.5 * IQR)) & (col("salary") <= (Q3 + 1.5 * IQR)))

    print("DataFrame after removing outliers in 'salary':")
    df.show(truncate=False)

    return df


def data_reduction(df):
    assembler = VectorAssembler(inputCols=["age", "salary"], outputCol="features")
    df_vector = assembler.transform(df)

    # MinMaxScaler
    minmax_scaler = MinMaxScaler(inputCol="features", outputCol="minmax_scaled")
    minmax_model = minmax_scaler.fit(df_vector)
    df_minmax = minmax_model.transform(df_vector)

    # StandardScaler
    standard_scaler = StandardScaler(inputCol="features", outputCol="standard_scaled", withMean=True, withStd=True)
    standard_model = standard_scaler.fit(df_minmax)
    df_standard = standard_model.transform(df_minmax)

    # RobustScaler
    robust_scaler = RobustScaler(inputCol="features", outputCol="robust_scaled")
    robust_model = robust_scaler.fit(df_standard)
    df_robust = robust_model.transform(df_standard)

    print("DataFrame after scaling:")
    df_robust.select(
        "id", "name", "age", "salary",
        "minmax_scaled", "standard_scaled", "robust_scaled"
    ).show(truncate=False)


def main():
    data_profiling(df)
    cleaned_df = data_cleaning(df)
    data_reduction(cleaned_df)
    spark.stop()


if __name__ == "__main__":
    main()
